{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba69f3bc-25bb-4f25-ab5b-a2e1f07b7361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in d:\\phishguard-ai\\.venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in d:\\phishguard-ai\\.venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: scikit-learn in d:\\phishguard-ai\\.venv\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\phishguard-ai\\.venv\\lib\\site-packages (4.14.2)\n",
      "Requirement already satisfied: lxml in d:\\phishguard-ai\\.venv\\lib\\site-packages (6.0.2)\n",
      "Requirement already satisfied: tldextract in d:\\phishguard-ai\\.venv\\lib\\site-packages (5.3.0)\n",
      "Requirement already satisfied: python-dateutil in d:\\phishguard-ai\\.venv\\lib\\site-packages (2.9.0.post0)\n",
      "Requirement already satisfied: joblib in d:\\phishguard-ai\\.venv\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: matplotlib in d:\\phishguard-ai\\.venv\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: seaborn in d:\\phishguard-ai\\.venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: language-tool-python in d:\\phishguard-ai\\.venv\\lib\\site-packages (2.9.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: idna in d:\\phishguard-ai\\.venv\\lib\\site-packages (from tldextract) (3.11)\n",
      "Requirement already satisfied: requests>=2.1.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from tldextract) (2.32.5)\n",
      "Requirement already satisfied: requests-file>=1.4 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from tldextract) (2.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from tldextract) (3.20.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from python-dateutil) (1.17.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: tqdm in d:\\phishguard-ai\\.venv\\lib\\site-packages (from language-tool-python) (4.67.1)\n",
      "Requirement already satisfied: psutil in d:\\phishguard-ai\\.venv\\lib\\site-packages (from language-tool-python) (7.1.0)\n",
      "Requirement already satisfied: toml in d:\\phishguard-ai\\.venv\\lib\\site-packages (from language-tool-python) (0.10.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from requests>=2.1.0->tldextract) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from requests>=2.1.0->tldextract) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from requests>=2.1.0->tldextract) (2025.10.5)\n",
      "Requirement already satisfied: colorama in d:\\phishguard-ai\\.venv\\lib\\site-packages (from tqdm->language-tool-python) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy scikit-learn beautifulsoup4 lxml tldextract python-dateutil joblib matplotlib seaborn language-tool-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31dd3dba-ae09-4e99-9b3a-fad6e3ccccba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in d:\\phishguard-ai\\.venv\\lib\\site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from spacy) (0.19.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from spacy) (2.3.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from spacy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from spacy) (2.12.2)\n",
      "Requirement already satisfied: jinja2 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in d:\\phishguard-ai\\.venv\\lib\\site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in d:\\phishguard-ai\\.venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.2.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from jinja2->spacy) (3.0.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: wrapt in d:\\phishguard-ai\\.venv\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67005ae6-761c-47a5-95aa-9d0566ebbefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers in d:\\phishguard-ai\\.venv\\lib\\site-packages (5.1.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from sentence_transformers) (4.57.1)\n",
      "Requirement already satisfied: tqdm in d:\\phishguard-ai\\.venv\\lib\\site-packages (from sentence_transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from sentence_transformers) (2.9.0)\n",
      "Requirement already satisfied: scikit-learn in d:\\phishguard-ai\\.venv\\lib\\site-packages (from sentence_transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in d:\\phishguard-ai\\.venv\\lib\\site-packages (from sentence_transformers) (1.16.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from sentence_transformers) (0.35.3)\n",
      "Requirement already satisfied: Pillow in d:\\phishguard-ai\\.venv\\lib\\site-packages (from sentence_transformers) (12.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from sentence_transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in d:\\phishguard-ai\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.3)\n",
      "Requirement already satisfied: requests in d:\\phishguard-ai\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.5)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in d:\\phishguard-ai\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (80.9.0)\n",
      "Requirement already satisfied: colorama in d:\\phishguard-ai\\.venv\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.3.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2025.9.18)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.6.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from scikit-learn->sentence_transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2025.10.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3df51ab-97fe-4266-b1f3-89ffa0e7836f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\PhishGuard-AI\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import mailbox\n",
    "import email\n",
    "from email import policy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import language_tool_python\n",
    "from urllib.parse import urlparse\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import joblib  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5999a9d4-8da5-47ab-bbd9-1c7a0e75e50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SHORTENER_DOMAINS = {\"bit.ly\", \"tinyurl.com\", \"t.co\", \"goo.gl\", \"ow.ly\", \"is.gd\", \"buff.ly\"}\n",
    "SUSPICIOUS_EXTS = {'.exe', '.scr', '.pif', '.jar', '.js', '.vbs', '.html', '.hta', '.docm', '.xlsm', '.xls', '.pptm', '.zip', '.rar'}\n",
    "\n",
    "# Extract text parts and attachments\n",
    "def get_text_parts(msg):\n",
    "    txt_parts = []\n",
    "    html_parts = []\n",
    "    attachments = []\n",
    "    for part in msg.walk():\n",
    "        ctype = part.get_content_type()\n",
    "        fn = part.get_filename()\n",
    "        if fn:\n",
    "            attachments.append(fn)\n",
    "        if ctype == \"text/plain\" and not fn:\n",
    "            try:\n",
    "                txt_parts.append(part.get_payload(decode=True).decode(errors='ignore'))\n",
    "            except:\n",
    "                pass\n",
    "        elif ctype == \"text/html\" and not fn:\n",
    "            try:\n",
    "                html_parts.append(part.get_payload(decode=True).decode(errors='ignore'))\n",
    "            except:\n",
    "                pass\n",
    "    if not txt_parts and not html_parts:\n",
    "        try:\n",
    "            payload = msg.get_payload(decode=True)\n",
    "            if isinstance(payload, bytes):\n",
    "                txt_parts.append(payload.decode(errors='ignore'))\n",
    "        except:\n",
    "            pass\n",
    "    return \"\\n\".join(txt_parts), \"\\n\".join(html_parts), attachments\n",
    "\n",
    "# URL extraction\n",
    "URL_REGEX = re.compile(r\"https?://[^\\s'\\\"<>]+\", re.IGNORECASE)\n",
    "IP_IN_URL = re.compile(r\"https?://(?:\\d{1,3}\\.){3}\\d{1,3}\")\n",
    "\n",
    "def extract_urls_from_html(html):\n",
    "    urls = []\n",
    "    anchors = []\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    for a in soup.find_all(\"a\"):\n",
    "        href = a.get('href')\n",
    "        text = (a.get_text() or \"\").strip()\n",
    "        if href:\n",
    "            urls.append(href)\n",
    "            anchors.append((href, text))\n",
    "    for m in URL_REGEX.finditer(html):\n",
    "        urls.append(m.group(0))\n",
    "    return urls, anchors\n",
    "\n",
    "# Parse mbox files\n",
    "def parse_mbox(mbox_path, label, limit=None):\n",
    "    m = mailbox.mbox(mbox_path)\n",
    "    rows = []\n",
    "    for i, msg in enumerate(m):\n",
    "        if limit and i >= limit: break\n",
    "        try:\n",
    "            raw = msg.as_string()\n",
    "            parsed = email.message_from_string(raw, policy=policy.default)\n",
    "            subj = parsed.get(\"subject\", \"\") or \"\"\n",
    "            frm = parsed.get(\"from\", \"\") or \"\"\n",
    "            retpath = parsed.get(\"return-path\", \"\") or parsed.get(\"Return-Path\", \"\") or \"\"\n",
    "            date = parsed.get(\"date\", \"\") or \"\"\n",
    "            try:\n",
    "                date_parsed = dateparser.parse(date) if date else None\n",
    "            except:\n",
    "                date_parsed = None\n",
    "            body_txt, body_html, attachments = get_text_parts(parsed)\n",
    "            urls, anchors = extract_urls_from_html(body_html or body_txt)\n",
    "            rows.append({\n",
    "                \"subject\": subj,\n",
    "                \"from\": frm,\n",
    "                \"return_path\": retpath,\n",
    "                \"date\": date_parsed,\n",
    "                \"body_text\": body_txt,\n",
    "                \"body_html\": body_html,\n",
    "                \"urls\": urls,\n",
    "                \"anchors\": anchors,\n",
    "                \"attachments\": attachments,\n",
    "                \"label\": label\n",
    "            })\n",
    "        except LookupError as e:\n",
    "            print(f\"Skipping email {i} in {mbox_path} due to encoding error: {e}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping email {i} in {mbox_path} due to general error: {e}\")\n",
    "            continue\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Parse raw directory files\n",
    "def parse_raw_dir(dir_path, label, limit=None):\n",
    "    rows = []\n",
    "    files = []\n",
    "    for root, _, filenames in os.walk(dir_path):\n",
    "        for fn in filenames:\n",
    "            files.append(os.path.join(root, fn))\n",
    "    for i, path in enumerate(files):\n",
    "        if limit and i >= limit: break\n",
    "        try:\n",
    "            with open(path, \"rb\") as f:\n",
    "                msg = email.message_from_binary_file(f, policy=policy.default)\n",
    "        except:\n",
    "            try:\n",
    "                with open(path, \"r\", errors=\"ignore\") as f:\n",
    "                    msg = email.message_from_string(f.read(), policy=policy.default)\n",
    "            except:\n",
    "                continue\n",
    "        subj = msg.get(\"subject\", \"\") or \"\"\n",
    "        frm = msg.get(\"from\", \"\") or \"\"\n",
    "        retpath = msg.get(\"return-path\", \"\") or msg.get(\"Return-Path\", \"\") or \"\"\n",
    "        date = msg.get(\"date\", \"\") or \"\"\n",
    "        try:\n",
    "            date_parsed = dateparser.parse(date) if date else None\n",
    "        except:\n",
    "            date_parsed = None\n",
    "        body_txt, body_html, attachments = get_text_parts(msg)\n",
    "        urls, anchors = extract_urls_from_html(body_html or body_txt)\n",
    "        rows.append({\n",
    "            \"subject\": subj,\n",
    "            \"from\": frm,\n",
    "            \"return_path\": retpath,\n",
    "            \"date\": date_parsed,\n",
    "            \"body_text\": body_txt,\n",
    "            \"body_html\": body_html,\n",
    "            \"urls\": urls,\n",
    "            \"anchors\": anchors,\n",
    "            \"attachments\": attachments,\n",
    "            \"label\": label\n",
    "        })\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a3ad117-424d-4658-b4bc-c4d9776a1b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping email 1318 in D:\\data\\phishing2.mbox due to encoding error: unknown encoding: iso-18899997-1\n",
      "label\n",
      "0    3052\n",
      "1    1878\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load SpamAssassin ham (0)\n",
    "df_ham = parse_raw_dir(r\"D:\\data\\20021010_easy_ham\", label=0)\n",
    "\n",
    "# Load SpamAssassin spam (0 as well, since it's \"non-phishing spam\")\n",
    "df_spam = parse_raw_dir(r\"D:\\data\\20030228_spam\", label=0)\n",
    "\n",
    "# Load Nazario phishing (1)\n",
    "df_phish1 = parse_mbox(r\"D:\\data\\phishing1.mbox\", label=1)\n",
    "df_phish2 = parse_mbox(r\"D:\\data\\phishing2.mbox\", label=1)\n",
    "\n",
    "# Combine\n",
    "df = pd.concat([df_ham, df_spam, df_phish1, df_phish2], ignore_index=True)\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad16d97e-0c6b-4f8f-a070-d9f5b33dc5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4930 [00:00<?, ?it/s]C:\\Users\\froze\\AppData\\Local\\Temp\\ipykernel_20968\\1811965851.py:19: DeprecationWarning: The 'registered_domain' property is deprecated and will be removed in the next major version. Use 'top_domain_under_public_suffix' instead, which has the same behavior but a more accurate name.\n",
      "  return tldextract.extract(u).registered_domain\n",
      "C:\\Users\\froze\\AppData\\Local\\Temp\\ipykernel_20968\\1811965851.py:15: DeprecationWarning: The 'registered_domain' property is deprecated and will be removed in the next major version. Use 'top_domain_under_public_suffix' instead, which has the same behavior but a more accurate name.\n",
      "  return tldextract.extract(dom).registered_domain or dom\n",
      "  1%|▏         | 66/4930 [00:00<00:16, 300.82it/s]C:\\Users\\froze\\AppData\\Local\\Temp\\ipykernel_20968\\1811965851.py:61: DeprecationWarning: The 'registered_domain' property is deprecated and will be removed in the next major version. Use 'top_domain_under_public_suffix' instead, which has the same behavior but a more accurate name.\n",
      "  tdom = tldextract.extract(m.group(1)).registered_domain\n",
      "100%|██████████| 4930/4930 [00:08<00:00, 590.44it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>from</th>\n",
       "      <th>return_path</th>\n",
       "      <th>date</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_html</th>\n",
       "      <th>urls</th>\n",
       "      <th>anchors</th>\n",
       "      <th>attachments</th>\n",
       "      <th>label</th>\n",
       "      <th>...</th>\n",
       "      <th>kw_urgent</th>\n",
       "      <th>kw_click here</th>\n",
       "      <th>kw_bank</th>\n",
       "      <th>kw_login</th>\n",
       "      <th>kw_update</th>\n",
       "      <th>kw_suspend</th>\n",
       "      <th>kw_confirm</th>\n",
       "      <th>kw_security</th>\n",
       "      <th>kw_ssn</th>\n",
       "      <th>kw_credit card</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Re: New Sequences Window</td>\n",
       "      <td>Robert Elz &lt;kre@munnari.OZ.AU&gt;</td>\n",
       "      <td>&lt;exmh-workers-admin@example.com&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>Date:        Wed, 21 Aug 2002 10:54:46 -05...</td>\n",
       "      <td></td>\n",
       "      <td>[https://listman.redhat.com/mailman/listinfo/e...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[zzzzteana] RE: Alexander</td>\n",
       "      <td>Steve Burt &lt;Steve_Burt@cursor-system.com&gt;</td>\n",
       "      <td>&lt;Steve_Burt@cursor-system.com&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>Martin A posted:\\nTassos Papadopoulos, the Gre...</td>\n",
       "      <td></td>\n",
       "      <td>[http://us.click.yahoo.com/pt6YBB/NXiEAA/mG3HA...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[zzzzteana] Moscow bomber</td>\n",
       "      <td>Tim Chapman &lt;timc@2ubh.com&gt;</td>\n",
       "      <td>&lt;timc@2ubh.com&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>Man Threatens Explosion In Moscow \\n\\nThursday...</td>\n",
       "      <td></td>\n",
       "      <td>[http://us.click.yahoo.com/pt6YBB/NXiEAA/mG3HA...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[IRR] Klez: The Virus That  Won't Die</td>\n",
       "      <td>Monty Solomon &lt;monty@roscom.com&gt;</td>\n",
       "      <td>&lt;irregulars-admin@tb.tf&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>Klez: The Virus That Won't Die\\n \\nAlready the...</td>\n",
       "      <td></td>\n",
       "      <td>[http://www.pcworld.com/news/article/0,aid,103...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Re: Insert signature</td>\n",
       "      <td>Tony Nugent &lt;tony@linuxworks.com.au&gt;</td>\n",
       "      <td>&lt;exmh-users-admin@example.com&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>On Wed Aug 21 2002 at 15:46, Ulises Ponce wrot...</td>\n",
       "      <td></td>\n",
       "      <td>[https://listman.redhat.com/mailman/listinfo/e...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3795</th>\n",
       "      <td>Renewal Account</td>\n",
       "      <td>Commonwealth Renewal &lt;renewal@commbank.com.au&gt;</td>\n",
       "      <td>&lt;nobody@galaxy.frogee.com&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>\\n&lt;HTML&gt;\\n&lt;HEAD&gt;\\n&lt;TITLE&gt;Commonwealth Bank of ...</td>\n",
       "      <td>[http://customer-148-235-82-226.uninet-ide.com...</td>\n",
       "      <td>[(http://customer-148-235-82-226.uninet-ide.co...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3796</th>\n",
       "      <td>Chase Bank Online� Department Notice</td>\n",
       "      <td>\"Chase Bank Online� Department &lt;service@chase....</td>\n",
       "      <td>&lt;nobody@server1.websbesthosting.com&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>&lt;html&gt;\\n&lt;xbody&gt;\\n&lt;img src=\"http://www.chase.co...</td>\n",
       "      <td>[http://chaseonline.chase.com.chaseonline.8k.c...</td>\n",
       "      <td>[(http://chaseonline.chase.com.chaseonline.8k....</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3797</th>\n",
       "      <td>You have successfully added a new email addres...</td>\n",
       "      <td>PayPal &lt;do-not-reply@paypal.com&gt;</td>\n",
       "      <td>&lt;do-not-reply@paypal.com&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;title&gt;PayPal&lt;/title&gt;&lt;/head&gt;&lt;body&gt;...</td>\n",
       "      <td>[http://rds.yahoo.com/S=44831148:D1/CS=4483114...</td>\n",
       "      <td>[(http://rds.yahoo.com/S=44831148:D1/CS=448311...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3798</th>\n",
       "      <td>Account Security Measures</td>\n",
       "      <td>\"service@AACreditUnion.org.com\" &lt;service@AACre...</td>\n",
       "      <td>&lt;root@ent-club.com&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>\\n&lt;html&gt;\\n\\n\\n\\n\\n\\n&lt;!-- The following is the ...</td>\n",
       "      <td>[http://svr2.wmsys.net/www.aacreditunion.org/c...</td>\n",
       "      <td>[(http://svr2.wmsys.net/www.aacreditunion.org/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3799</th>\n",
       "      <td>You have successfully added a new email addres...</td>\n",
       "      <td>PayPal &lt;do-not-reply@paypal.com.192-168-0-1.be&gt;</td>\n",
       "      <td>&lt;do-not-reply@paypal.com.192-168-0-1.be&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;title&gt;PayPal&lt;/title&gt;&lt;/head&gt;&lt;body&gt;...</td>\n",
       "      <td>[http://rds.yahoo.com/S=44831148:D1/CS=4483114...</td>\n",
       "      <td>[(http://rds.yahoo.com/S=44831148:D1/CS=448311...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3800 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                subject  \\\n",
       "0                              Re: New Sequences Window   \n",
       "1                             [zzzzteana] RE: Alexander   \n",
       "2                             [zzzzteana] Moscow bomber   \n",
       "3                 [IRR] Klez: The Virus That  Won't Die   \n",
       "4                                  Re: Insert signature   \n",
       "...                                                 ...   \n",
       "3795                                    Renewal Account   \n",
       "3796               Chase Bank Online� Department Notice   \n",
       "3797  You have successfully added a new email addres...   \n",
       "3798                          Account Security Measures   \n",
       "3799  You have successfully added a new email addres...   \n",
       "\n",
       "                                                   from  \\\n",
       "0                        Robert Elz <kre@munnari.OZ.AU>   \n",
       "1             Steve Burt <Steve_Burt@cursor-system.com>   \n",
       "2                           Tim Chapman <timc@2ubh.com>   \n",
       "3                      Monty Solomon <monty@roscom.com>   \n",
       "4                  Tony Nugent <tony@linuxworks.com.au>   \n",
       "...                                                 ...   \n",
       "3795     Commonwealth Renewal <renewal@commbank.com.au>   \n",
       "3796  \"Chase Bank Online� Department <service@chase....   \n",
       "3797                   PayPal <do-not-reply@paypal.com>   \n",
       "3798  \"service@AACreditUnion.org.com\" <service@AACre...   \n",
       "3799    PayPal <do-not-reply@paypal.com.192-168-0-1.be>   \n",
       "\n",
       "                                   return_path  date  \\\n",
       "0             <exmh-workers-admin@example.com>  None   \n",
       "1               <Steve_Burt@cursor-system.com>  None   \n",
       "2                              <timc@2ubh.com>  None   \n",
       "3                     <irregulars-admin@tb.tf>  None   \n",
       "4               <exmh-users-admin@example.com>  None   \n",
       "...                                        ...   ...   \n",
       "3795                <nobody@galaxy.frogee.com>  None   \n",
       "3796      <nobody@server1.websbesthosting.com>  None   \n",
       "3797                 <do-not-reply@paypal.com>  None   \n",
       "3798                       <root@ent-club.com>  None   \n",
       "3799  <do-not-reply@paypal.com.192-168-0-1.be>  None   \n",
       "\n",
       "                                              body_text  \\\n",
       "0         Date:        Wed, 21 Aug 2002 10:54:46 -05...   \n",
       "1     Martin A posted:\\nTassos Papadopoulos, the Gre...   \n",
       "2     Man Threatens Explosion In Moscow \\n\\nThursday...   \n",
       "3     Klez: The Virus That Won't Die\\n \\nAlready the...   \n",
       "4     On Wed Aug 21 2002 at 15:46, Ulises Ponce wrot...   \n",
       "...                                                 ...   \n",
       "3795                                                      \n",
       "3796                                                      \n",
       "3797                                                      \n",
       "3798                                                      \n",
       "3799                                                      \n",
       "\n",
       "                                              body_html  \\\n",
       "0                                                         \n",
       "1                                                         \n",
       "2                                                         \n",
       "3                                                         \n",
       "4                                                         \n",
       "...                                                 ...   \n",
       "3795  \\n<HTML>\\n<HEAD>\\n<TITLE>Commonwealth Bank of ...   \n",
       "3796  <html>\\n<xbody>\\n<img src=\"http://www.chase.co...   \n",
       "3797  <html><head><title>PayPal</title></head><body>...   \n",
       "3798  \\n<html>\\n\\n\\n\\n\\n\\n<!-- The following is the ...   \n",
       "3799  <html><head><title>PayPal</title></head><body>...   \n",
       "\n",
       "                                                   urls  \\\n",
       "0     [https://listman.redhat.com/mailman/listinfo/e...   \n",
       "1     [http://us.click.yahoo.com/pt6YBB/NXiEAA/mG3HA...   \n",
       "2     [http://us.click.yahoo.com/pt6YBB/NXiEAA/mG3HA...   \n",
       "3     [http://www.pcworld.com/news/article/0,aid,103...   \n",
       "4     [https://listman.redhat.com/mailman/listinfo/e...   \n",
       "...                                                 ...   \n",
       "3795  [http://customer-148-235-82-226.uninet-ide.com...   \n",
       "3796  [http://chaseonline.chase.com.chaseonline.8k.c...   \n",
       "3797  [http://rds.yahoo.com/S=44831148:D1/CS=4483114...   \n",
       "3798  [http://svr2.wmsys.net/www.aacreditunion.org/c...   \n",
       "3799  [http://rds.yahoo.com/S=44831148:D1/CS=4483114...   \n",
       "\n",
       "                                                anchors attachments  label  \\\n",
       "0                                                    []          []      0   \n",
       "1                                                    []          []      0   \n",
       "2                                                    []          []      0   \n",
       "3                                                    []          []      0   \n",
       "4                                                    []          []      0   \n",
       "...                                                 ...         ...    ...   \n",
       "3795  [(http://customer-148-235-82-226.uninet-ide.co...          []      1   \n",
       "3796  [(http://chaseonline.chase.com.chaseonline.8k....          []      1   \n",
       "3797  [(http://rds.yahoo.com/S=44831148:D1/CS=448311...          []      1   \n",
       "3798  [(http://svr2.wmsys.net/www.aacreditunion.org/...          []      1   \n",
       "3799  [(http://rds.yahoo.com/S=44831148:D1/CS=448311...          []      1   \n",
       "\n",
       "      ...  kw_urgent  kw_click here  kw_bank  kw_login  kw_update  kw_suspend  \\\n",
       "0     ...          0              0        0         0          0           0   \n",
       "1     ...          0              0        0         0          0           0   \n",
       "2     ...          0              0        0         0          0           0   \n",
       "3     ...          0              0        0         0          0           0   \n",
       "4     ...          0              0        0         0          0           0   \n",
       "...   ...        ...            ...      ...       ...        ...         ...   \n",
       "3795  ...          0              0        0         0          0           0   \n",
       "3796  ...          0              0        1         0          0           0   \n",
       "3797  ...          0              0        0         0          0           0   \n",
       "3798  ...          0              0        0         0          0           0   \n",
       "3799  ...          0              0        0         0          0           0   \n",
       "\n",
       "      kw_confirm  kw_security  kw_ssn  kw_credit card  \n",
       "0              0            0       0               0  \n",
       "1              0            0       0               0  \n",
       "2              0            5       0               0  \n",
       "3              0            1       0               0  \n",
       "4              0            0       0               0  \n",
       "...          ...          ...     ...             ...  \n",
       "3795           0            0       0               0  \n",
       "3796           0            0       0               0  \n",
       "3797           0            0       0               0  \n",
       "3798           0            1       0               0  \n",
       "3799           0            0       0               0  \n",
       "\n",
       "[3800 rows x 42 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tldextract\n",
    "KEYWORDS = [\"verify\",\"password\",\"account\",\"urgent\",\"click here\",\"bank\",\"login\",\"update\",\"suspend\",\"confirm\",\"security\",\"ssn\",\"credit card\"]\n",
    "\n",
    "def domain_of_address(addr):\n",
    "    # extract registered domain from an email \"Name <a@b.com>\"\n",
    "    if not addr:\n",
    "        return \"\"\n",
    "    # naive extract of email portion\n",
    "    m = re.search(r\"[\\w\\.-]+@([\\w\\.-]+)\", addr)\n",
    "    if m:\n",
    "        dom = m.group(1)\n",
    "    else:\n",
    "        # maybe domain present directly\n",
    "        dom = addr.strip()\n",
    "    return tldextract.extract(dom).registered_domain or dom\n",
    "\n",
    "def url_domain(u):\n",
    "    try:\n",
    "        return tldextract.extract(u).registered_domain\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def extract_features_row(r):\n",
    "    subj = r.get(\"subject\",\"\") or \"\"\n",
    "    body = r.get(\"body_text\",\"\") or \"\"\n",
    "    fulltext = (subj + \" \" + body).strip()\n",
    "    urls = r.get(\"urls\") or []\n",
    "    anchors = r.get(\"anchors\") or []\n",
    "    attachments = r.get(\"attachments\") or []\n",
    "\n",
    "    # Lexical\n",
    "    subj_len = len(subj)\n",
    "    subj_words = len(subj.split())\n",
    "    body_len = len(body)\n",
    "    body_words = len(body.split())\n",
    "    letters = sum(1 for c in fulltext if c.isalpha())\n",
    "    uppercase = sum(1 for c in fulltext if c.isupper())\n",
    "    uppercase_ratio = uppercase / max(1, letters)\n",
    "    exclaim_count = fulltext.count(\"!\")\n",
    "    question_count = fulltext.count(\"?\")\n",
    "\n",
    "    # keywords\n",
    "    kw_counts = {f\"kw_{k}\": fulltext.lower().count(k) for k in KEYWORDS}\n",
    "\n",
    "    # URL features\n",
    "    num_urls = len(urls)\n",
    "    num_ip_urls = sum(1 for u in urls if IP_IN_URL.search(u))\n",
    "    num_shorteners = sum(1 for u in urls if urlparse(u).netloc.replace(\"www.\",\"\") in SHORTENER_DOMAINS)\n",
    "    avg_url_len = np.mean([len(u) for u in urls]) if urls else 0\n",
    "    unique_domains = {url_domain(u) for u in urls if url_domain(u)}\n",
    "    num_unique_domains = len(unique_domains)\n",
    "\n",
    "    # anchor mismatch: compare anchor text domain vs href domain (if anchor text contains a domain-like token)\n",
    "    domain_mismatch = False\n",
    "    for href, text in anchors:\n",
    "        tdom = None\n",
    "        hdom = url_domain(href)\n",
    "        # rough parse anchor text looking for domain\n",
    "        m = re.search(r\"([\\w\\.-]+\\.[a-z]{2,})\", text)\n",
    "        if m:\n",
    "            tdom = tldextract.extract(m.group(1)).registered_domain\n",
    "        if tdom and hdom and tdom != hdom:\n",
    "            domain_mismatch = True\n",
    "            break\n",
    "\n",
    "    # header features\n",
    "# header features\n",
    "    from_domain = domain_of_address(r.get(\"from\",\"\"))\n",
    "    ret_domain = domain_of_address(r.get(\"return_path\",\"\"))\n",
    "    from_return_mismatch = bool(from_domain and ret_domain and from_domain != ret_domain)\n",
    "\n",
    "\n",
    "    auth_headers = r.get(\"body_html\",\"\") + r.get(\"body_text\",\"\")\n",
    "    # crude presence check (not authoritative)\n",
    "    has_spf = bool(re.search(r\"spf\", auth_headers, re.I))\n",
    "    has_dkim = bool(re.search(r\"dkim\", auth_headers, re.I))\n",
    "\n",
    "    # attachments\n",
    "    num_attachments = len(attachments)\n",
    "    has_suspicious_attachment = any(os.path.splitext(fn)[1].lower() in SUSPICIOUS_EXTS for fn in attachments)\n",
    "\n",
    "    feats = dict(\n",
    "        subj_len=subj_len, subj_words=subj_words,\n",
    "        body_len=body_len, body_words=body_words,\n",
    "        uppercase_ratio=uppercase_ratio,\n",
    "        exclaim_count=exclaim_count, question_count=question_count,\n",
    "        num_urls=num_urls, num_ip_urls=num_ip_urls,\n",
    "        num_shorteners=num_shorteners, avg_url_len=avg_url_len,\n",
    "        num_unique_domains=num_unique_domains,\n",
    "        domain_mismatch=int(domain_mismatch),\n",
    "        from_return_mismatch=int(from_return_mismatch),\n",
    "        has_spf=int(has_spf), has_dkim=int(has_dkim),\n",
    "        num_attachments=num_attachments, has_suspicious_attachment=int(has_suspicious_attachment),\n",
    "        text = fulltext  # keep text for vectorization\n",
    "    )\n",
    "    feats.update(kw_counts)\n",
    "    return feats\n",
    "\n",
    "# apply to DataFrame (can be slow; use .apply with progress)\n",
    "from tqdm import tqdm\n",
    "rows = []\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    rows.append(extract_features_row(row))\n",
    "feats_df = pd.DataFrame(rows)\n",
    "final_df = pd.concat([df.reset_index(drop=True), feats_df.reset_index(drop=True)], axis=1)\n",
    "final_df.head(3800)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2014633-6e32-478e-a5ca-596a9265a3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "text_col = \"text\"\n",
    "numeric_cols = [\n",
    "    \"subj_len\",\"subj_words\",\"body_len\",\"body_words\",\"uppercase_ratio\",\n",
    "    \"exclaim_count\",\"question_count\",\"num_urls\",\"num_ip_urls\",\"num_shorteners\",\n",
    "    \"avg_url_len\",\"num_unique_domains\",\"domain_mismatch\",\"from_return_mismatch\",\n",
    "    \"has_spf\",\"has_dkim\",\"num_attachments\",\"has_suspicious_attachment\"\n",
    "] + [f\"kw_{k}\" for k in KEYWORDS]\n",
    "\n",
    "# fill missing numeric cols (if any) and to numeric\n",
    "final_df[numeric_cols] = final_df[numeric_cols].fillna(0).astype(float)\n",
    "\n",
    "# TF-IDF (word n-grams + char n-grams optional)\n",
    "vectorizer = TfidfVectorizer(max_features=20000, ngram_range=(1,2), min_df=3)\n",
    "X_text = vectorizer.fit_transform(final_df[text_col].fillna(\"\"))\n",
    "\n",
    "scaler = StandardScaler(with_mean=False)  # with_mean False because we'll combine with sparse matrix\n",
    "X_num = scaler.fit_transform(final_df[numeric_cols].values)  # dense\n",
    "\n",
    "# combine into sparse X\n",
    "X = hstack([X_text, csr_matrix(X_num)])\n",
    "y = final_df['label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66c8a6cb-dd0a-4925-8caf-7251c994a829",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\PhishGuard-AI\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9601    0.9475    0.9538       610\n",
      "           1     0.9167    0.9362    0.9263       376\n",
      "\n",
      "    accuracy                         0.9432       986\n",
      "   macro avg     0.9384    0.9419    0.9401       986\n",
      "weighted avg     0.9436    0.9432    0.9433       986\n",
      "\n",
      "ROC AUC: 0.9856949773282175\n",
      "RF report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9720    0.9689    0.9704       610\n",
      "           1     0.9497    0.9548    0.9523       376\n",
      "\n",
      "    accuracy                         0.9635       986\n",
      "   macro avg     0.9609    0.9618    0.9613       986\n",
      "weighted avg     0.9635    0.9635    0.9635       986\n",
      "\n",
      "ROC AUC: 0.9959321590512731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\PhishGuard-AI\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensemble report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9752    0.9656    0.9703       610\n",
      "           1     0.9450    0.9601    0.9525       376\n",
      "\n",
      "    accuracy                         0.9635       986\n",
      "   macro avg     0.9601    0.9628    0.9614       986\n",
      "weighted avg     0.9637    0.9635    0.9635       986\n",
      "\n",
      "ROC AUC: 0.9963376351587024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression (baseline)\n",
    "clf_lr = LogisticRegression(max_iter=2000, class_weight='balanced', solver='saga')\n",
    "clf_lr.fit(X_train, y_train)\n",
    "probs_lr = clf_lr.predict_proba(X_test)[:,1]\n",
    "preds_lr = clf_lr.predict(X_test)\n",
    "\n",
    "print(\"LogReg report:\")\n",
    "print(classification_report(y_test, preds_lr, digits=4))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, probs_lr))\n",
    "\n",
    "# Random Forest\n",
    "clf_rf = RandomForestClassifier(n_estimators=200, class_weight='balanced', n_jobs=-1, random_state=42)\n",
    "clf_rf.fit(X_train, y_train)\n",
    "preds_rf = clf_rf.predict(X_test)\n",
    "probs_rf = clf_rf.predict_proba(X_test)[:,1]\n",
    "print(\"RF report:\")\n",
    "print(classification_report(y_test, preds_rf, digits=4))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, probs_rf))\n",
    "\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Create ensemble with soft voting\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        (\"lr\", clf_lr),\n",
    "        (\"rf\", clf_rf)\n",
    "    ],\n",
    "    voting=\"soft\",   # uses predict_proba instead of hard class majority\n",
    "    weights=[1, 2]   # you can adjust (e.g. give more weight to RF if it's better)\n",
    ")\n",
    "\n",
    "# Fit ensemble\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "preds_ens = ensemble.predict(X_test)\n",
    "probs_ens = ensemble.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nEnsemble report:\")\n",
    "print(classification_report(y_test, preds_ens, digits=4))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, probs_ens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fb600fe-c84b-4850-a481-f388843c6efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top text tokens (phish): [(np.float64(2.579571989215233), 'account'), (np.float64(2.291822449792038), 'ebay'), (np.float64(2.1717944342141), 'notification'), (np.float64(1.8035026623416603), 'paypal'), (np.float64(1.4957880852576657), 'chase'), (np.float64(1.4929781397964499), 'security'), (np.float64(1.4191819497591485), 'your account'), (np.float64(1.3902492134929003), 'update'), (np.float64(1.3599307921837882), 'posible spam'), (np.float64(1.3599307921837882), 'posible')]\n",
      "Top text tokens (ham): [(np.float64(-2.9964757877675403), 'the'), (np.float64(-1.8987797167165756), '2002'), (np.float64(-1.8408728178149305), 'http'), (np.float64(-1.7581372125992736), 'http www'), (np.float64(-1.733826975415234), 'www'), (np.float64(-1.6417459582387726), 'url http'), (np.float64(-1.5976377370664263), 'to'), (np.float64(-1.5687139551245934), 'url'), (np.float64(-1.5503235397520567), 'for'), (np.float64(-1.464549434184205), 'free')]\n",
      "Top numeric features: [(np.float64(1.0537566049441627), 'domain_mismatch'), (np.float64(0.8804146573305045), 'kw_account'), (np.float64(0.719985862585083), 'kw_bank'), (np.float64(0.6379732047058434), 'kw_suspend'), (np.float64(0.6349558637777445), 'num_attachments'), (np.float64(0.564446680258734), 'subj_len'), (np.float64(0.5071824121414539), 'num_urls'), (np.float64(0.41159493694937976), 'kw_login'), (np.float64(0.4096813301041749), 'avg_url_len'), (np.float64(0.33653766715154143), 'kw_confirm')]\n"
     ]
    }
   ],
   "source": [
    "# feature names\n",
    "text_names = vectorizer.get_feature_names_out()\n",
    "all_feature_names = list(text_names) + numeric_cols\n",
    "\n",
    "coefs = clf_lr.coef_[0]\n",
    "# top positive tokens\n",
    "top_pos = sorted(zip(coefs[:len(text_names)], text_names), reverse=True)[:20]\n",
    "top_neg = sorted(zip(coefs[:len(text_names)], text_names))[:20]\n",
    "print(\"Top text tokens (phish):\", top_pos[:10])\n",
    "print(\"Top text tokens (ham):\", top_neg[:10])\n",
    "\n",
    "# numeric feature importance (approx)\n",
    "num_coefs = coefs[len(text_names):]\n",
    "num_feat_imp = sorted(zip(num_coefs, numeric_cols), reverse=True)\n",
    "print(\"Top numeric features:\", num_feat_imp[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4496e20-d299-439c-b1ae-7febf6c275b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['phish_detector_joblib.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump({\n",
    "    \"vectorizer\": vectorizer,\n",
    "    \"scaler\": scaler,\n",
    "    \"numeric_cols\": numeric_cols,\n",
    "    \"model\": ensemble\n",
    "}, \"phish_detector_joblib.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfacc006-f746-45f4-a31c-e23505bd42f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.ensemble._voting.VotingClassifier'>\n",
      "VotingClassifier(estimators=[('lr',\n",
      "                              LogisticRegression(class_weight='balanced',\n",
      "                                                 max_iter=2000,\n",
      "                                                 solver='saga')),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(class_weight='balanced',\n",
      "                                                     n_estimators=200,\n",
      "                                                     n_jobs=-1,\n",
      "                                                     random_state=42))],\n",
      "                 voting='soft', weights=[1, 2])\n"
     ]
    }
   ],
   "source": [
    "assets = joblib.load(\"phish_detector_joblib.pkl\")\n",
    "print(type(assets[\"model\"]))\n",
    "print(assets[\"model\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae5e6332-bdc4-42f8-b349-15f9bcd3cdd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Email Classification:\n",
      "================================================================================\n",
      "\n",
      "--- Email 1 ---\n",
      "Warning: model not found. Using dummy predictions.\n",
      "Email snippet: Dear customer,\n",
      "    Your account has been locked due to suspicious activity.\n",
      "    ...\n",
      "Prediction: Phishing\n",
      "Probability (Benign): 0.300\n",
      "Probability (Phishing): 0.700\n",
      "Key features: {'uppercase_ratio': 0.025423728813559324, 'kw_verify': 1, 'num_urls': 1}\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Email 2 ---\n",
      "Warning: model not found. Using dummy predictions.\n",
      "Email snippet: Hi Alice,\n",
      "    Just a reminder about our team meeting tomorrow at 10 AM in the of...\n",
      "Prediction: Benign\n",
      "Probability (Benign): 0.800\n",
      "Probability (Phishing): 0.200\n",
      "Key features: {'uppercase_ratio': 0.09859154929577464}\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Email 3 ---\n",
      "Warning: model not found. Using dummy predictions.\n",
      "Email snippet: Congratulations! You've won a $1000 gift card.\n",
      "    Click here to claim: http://f...\n",
      "Prediction: Phishing\n",
      "Probability (Benign): 0.300\n",
      "Probability (Phishing): 0.700\n",
      "Key features: {'uppercase_ratio': 0.04225352112676056, 'kw_click_here': 1, 'num_urls': 1}\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Email 4 ---\n",
      "Warning: model not found. Using dummy predictions.\n",
      "Email snippet: Dear student,\n",
      "    Here is the link to download the lecture notes for tomorrow's ...\n",
      "Prediction: Benign\n",
      "Probability (Benign): 0.800\n",
      "Probability (Phishing): 0.200\n",
      "Key features: {'uppercase_ratio': 0.01904761904761905, 'num_urls': 1}\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Email 5 ---\n",
      "Warning: model not found. Using dummy predictions.\n",
      "Email snippet: Subject: Urgent: Verify Your Account Now\n",
      "\n",
      "Dear Customer,\n",
      "\n",
      "We noticed unusual act...\n",
      "Prediction: Phishing\n",
      "Probability (Benign): 0.300\n",
      "Probability (Phishing): 0.700\n",
      "Key features: {'uppercase_ratio': 0.05976095617529881, 'kw_verify': 4, 'kw_urgent': 1, 'num_urls': 1}\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Email 6 ---\n",
      "Warning: model not found. Using dummy predictions.\n",
      "Email snippet: Subject: Important: Confirm Recent Login\n",
      "\n",
      "Hi,\n",
      "\n",
      "We detected a login from a new de...\n",
      "Prediction: Benign\n",
      "Probability (Benign): 0.800\n",
      "Probability (Phishing): 0.200\n",
      "Key features: {'uppercase_ratio': 0.06395348837209303, 'num_urls': 1}\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Email 7 ---\n",
      "Warning: model not found. Using dummy predictions.\n",
      "Email snippet: Subject: Team Meeting Tomorrow — Agenda Attached\n",
      "\n",
      "Hi Team,\n",
      "\n",
      "Reminder: our weekly...\n",
      "Prediction: Benign\n",
      "Probability (Benign): 0.800\n",
      "Probability (Phishing): 0.200\n",
      "Key features: {'uppercase_ratio': 0.1342281879194631}\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Email 8 ---\n",
      "Warning: model not found. Using dummy predictions.\n",
      "Email snippet: Subject: Urgent: Verify Your Account Now\n",
      "\n",
      "Dear Customer,\n",
      "\n",
      "We detected unusual ac...\n",
      "Prediction: Phishing\n",
      "Probability (Benign): 0.300\n",
      "Probability (Phishing): 0.700\n",
      "Key features: {'uppercase_ratio': 0.1072961373390558, 'kw_verify': 2, 'kw_urgent': 1}\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Email 9 ---\n",
      "Warning: model not found. Using dummy predictions.\n",
      "Email snippet: Pranav Rh, Urgent Requirement for the role of Intern\n",
      "Inbox\n",
      "\n",
      "Naukri Campus Jobs <...\n",
      "Prediction: Phishing\n",
      "Probability (Benign): 0.300\n",
      "Probability (Phishing): 0.700\n",
      "Key features: {'uppercase_ratio': 0.06438467807660962, 'kw_urgent': 1}\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Email 10 ---\n",
      "Warning: model not found. Using dummy predictions.\n",
      "Email snippet: Subject: URGENT: Verify Your Account Immediately!\n",
      "From: support@secure-example.c...\n",
      "Prediction: Phishing\n",
      "Probability (Benign): 0.300\n",
      "Probability (Phishing): 0.700\n",
      "Key features: {'uppercase_ratio': 0.11538461538461539, 'kw_verify': 3, 'kw_urgent': 1, 'num_urls': 5}\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Email 11 ---\n",
      "Warning: model not found. Using dummy predictions.\n",
      "Email snippet: Subject: Urgent – Verify Your Account\n",
      "\n",
      "Dear user,  \n",
      "Your account has been flagge...\n",
      "Prediction: Phishing\n",
      "Probability (Benign): 0.300\n",
      "Probability (Phishing): 0.700\n",
      "Key features: {'uppercase_ratio': 0.07079646017699115, 'kw_verify': 2, 'kw_urgent': 1, 'num_urls': 1}\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import email\n",
    "from email import policy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "def preprocess_raw_email(raw_email_str):\n",
    "    \"\"\"Convert raw email string to features for prediction\"\"\"\n",
    "    \n",
    "    # Parse the email\n",
    "    try:\n",
    "        msg = email.message_from_string(raw_email_str, policy=policy.default)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing email: {e}\")\n",
    "        # Create dummy message for testing\n",
    "        msg = email.message.EmailMessage()\n",
    "        msg['subject'] = 'Test'\n",
    "        msg['from'] = 'test@example.com'\n",
    "        msg.set_content(raw_email_str)\n",
    "    \n",
    "    # Extract basic fields\n",
    "    subject = msg.get(\"subject\", \"\") or \"\"\n",
    "    from_ = msg.get(\"from\", \"\") or \"\"\n",
    "    return_path = msg.get(\"return-path\", \"\") or msg.get(\"Return-Path\", \"\") or \"\"\n",
    "    \n",
    "    # Extract body text\n",
    "    body_text = \"\"\n",
    "    body_html = \"\"\n",
    "    urls = []\n",
    "    attachments = []\n",
    "    \n",
    "    if msg.is_multipart():\n",
    "        for part in msg.walk():\n",
    "            ctype = part.get_content_type()\n",
    "            filename = part.get_filename()\n",
    "            \n",
    "            if filename:\n",
    "                attachments.append(filename)\n",
    "            \n",
    "            if ctype == \"text/plain\" and not filename:\n",
    "                try:\n",
    "                    payload = part.get_payload(decode=True)\n",
    "                    if payload:\n",
    "                        body_text += payload.decode('utf-8', errors='ignore') + \"\\n\"\n",
    "                except:\n",
    "                    pass\n",
    "            elif ctype == \"text/html\" and not filename:\n",
    "                try:\n",
    "                    payload = part.get_payload(decode=True)\n",
    "                    if payload:\n",
    "                        html_content = payload.decode('utf-8', errors='ignore')\n",
    "                        body_html += html_content + \"\\n\"\n",
    "                        # Extract URLs from HTML\n",
    "                        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "                        for link in soup.find_all('a', href=True):\n",
    "                            urls.append(link['href'])\n",
    "                except:\n",
    "                    pass\n",
    "    else:\n",
    "        try:\n",
    "            payload = msg.get_payload(decode=True)\n",
    "            if payload:\n",
    "                content = payload.decode('utf-8', errors='ignore')\n",
    "                if msg.get_content_type() == \"text/html\":\n",
    "                    body_html = content\n",
    "                    soup = BeautifulSoup(content, 'html.parser')\n",
    "                    for link in soup.find_all('a', href=True):\n",
    "                        urls.append(link['href'])\n",
    "                else:\n",
    "                    body_text = content\n",
    "        except:\n",
    "            # Fallback: treat as plain text\n",
    "            body_text = raw_email_str\n",
    "    \n",
    "    # If no structured body found, use raw email as body text\n",
    "    if not body_text and not body_html:\n",
    "        body_text = raw_email_str\n",
    "    \n",
    "    # Extract URLs from plain text using regex\n",
    "    URL_REGEX = re.compile(r\"https?://[^\\s'\\\"<>]+\", re.IGNORECASE)\n",
    "    urls.extend(URL_REGEX.findall(body_text))\n",
    "    urls = list(set(urls))  # Remove duplicates\n",
    "    \n",
    "    # Clean HTML to text if needed\n",
    "    if body_html and not body_text:\n",
    "        try:\n",
    "            soup = BeautifulSoup(body_html, 'html.parser')\n",
    "            body_text = soup.get_text()\n",
    "        except:\n",
    "            body_text = body_html\n",
    "    \n",
    "    # Combine text\n",
    "    full_text = (subject + \" \" + body_text).strip()\n",
    "    \n",
    "    # Initialize features dictionary\n",
    "    feats = {}\n",
    "    \n",
    "    # Text/lexical features\n",
    "    feats[\"subj_len\"] = len(subject)\n",
    "    feats[\"subj_words\"] = len(subject.split()) if subject else 0\n",
    "    feats[\"body_len\"] = len(body_text)\n",
    "    feats[\"body_words\"] = len(body_text.split()) if body_text else 0\n",
    "    \n",
    "    letters = sum(1 for c in full_text if c.isalpha())\n",
    "    uppercase = sum(1 for c in full_text if c.isupper())\n",
    "    feats[\"uppercase_ratio\"] = uppercase / max(1, letters)\n",
    "    feats[\"exclaim_count\"] = full_text.count(\"!\")\n",
    "    feats[\"question_count\"] = full_text.count(\"?\")\n",
    "    \n",
    "    # Keyword features\n",
    "    KEYWORDS = [\"verify\",\"password\",\"account\",\"urgent\",\"click here\",\"bank\",\"login\",\"update\",\"suspend\",\"confirm\",\"security\",\"ssn\",\"credit card\"]\n",
    "    for kw in KEYWORDS:\n",
    "        feats[f\"kw_{kw.replace(' ', '_')}\"] = full_text.lower().count(kw)\n",
    "    \n",
    "    # URL features\n",
    "    feats[\"num_urls\"] = len(urls)\n",
    "    \n",
    "    IP_IN_URL = re.compile(r\"https?://(?:\\d{1,3}\\.){3}\\d{1,3}\")\n",
    "    feats[\"num_ip_urls\"] = sum(1 for u in urls if IP_IN_URL.search(u))\n",
    "    \n",
    "    SHORTENER_DOMAINS = {\"bit.ly\", \"tinyurl.com\", \"t.co\", \"goo.gl\", \"ow.ly\", \"is.gd\", \"buff.ly\"}\n",
    "    feats[\"num_shorteners\"] = sum(1 for u in urls if urlparse(u).netloc.replace(\"www.\",\"\") in SHORTENER_DOMAINS)\n",
    "    \n",
    "    feats[\"avg_url_len\"] = np.mean([len(u) for u in urls]) if urls else 0\n",
    "    \n",
    "    # Domain features (simplified)\n",
    "    unique_domains = set()\n",
    "    for u in urls:\n",
    "        try:\n",
    "            domain = urlparse(u).netloc\n",
    "            if domain:\n",
    "                unique_domains.add(domain)\n",
    "        except:\n",
    "            pass\n",
    "    feats[\"num_unique_domains\"] = len(unique_domains)\n",
    "    \n",
    "    # Domain mismatch (simplified - just check if we have domains)\n",
    "    feats[\"domain_mismatch\"] = 0  # Simplified for testing\n",
    "    \n",
    "    # Header features\n",
    "    def simple_domain_extract(addr):\n",
    "        if not addr:\n",
    "            return \"\"\n",
    "        # Simple extraction without tldextract\n",
    "        match = re.search(r\"[\\w\\.-]+@([\\w\\.-]+)\", addr)\n",
    "        if match:\n",
    "            return match.group(1).lower()\n",
    "        return addr.lower().strip()\n",
    "    \n",
    "    from_domain = simple_domain_extract(from_)\n",
    "    return_domain = simple_domain_extract(return_path)\n",
    "    \n",
    "    # Fix the boolean logic\n",
    "    feats[\"from_return_mismatch\"] = int(bool(from_domain and return_domain and from_domain != return_domain))\n",
    "    \n",
    "    # Authentication (simplified)\n",
    "    feats[\"has_spf\"] = int(\"spf\" in full_text.lower())\n",
    "    feats[\"has_dkim\"] = int(\"dkim\" in full_text.lower())\n",
    "    \n",
    "    # Attachment features\n",
    "    feats[\"num_attachments\"] = len(attachments)\n",
    "    SUSPICIOUS_EXTS = {'.exe', '.scr', '.pif', '.jar', '.js', '.vbs', '.html', '.hta', '.docm', '.xlsm', '.xls', '.pptm', '.zip', '.rar'}\n",
    "    feats[\"has_suspicious_attachment\"] = int(any(\n",
    "        any(att.lower().endswith(ext) for ext in SUSPICIOUS_EXTS) \n",
    "        for att in attachments\n",
    "    ))\n",
    "    \n",
    "    # Ensure all numeric features are present\n",
    "    numeric_cols = [\n",
    "        \"subj_len\",\"subj_words\",\"body_len\",\"body_words\",\"uppercase_ratio\",\n",
    "        \"exclaim_count\",\"question_count\",\"num_urls\",\"num_ip_urls\",\"num_shorteners\",\n",
    "        \"avg_url_len\",\"num_unique_domains\",\"domain_mismatch\",\"from_return_mismatch\",\n",
    "        \"has_spf\",\"has_dkim\",\"num_attachments\",\"has_suspicious_attachment\"\n",
    "    ] + [f\"kw_{k.replace(' ', '_')}\" for k in KEYWORDS]\n",
    "    \n",
    "    # Fill missing features with 0\n",
    "    for col in numeric_cols:\n",
    "        if col not in feats:\n",
    "            feats[col] = 0\n",
    "    \n",
    "    # Create feature vector\n",
    "    numeric_vector = np.array([feats[col] for col in numeric_cols]).reshape(1, -1)\n",
    "    \n",
    "    # Transform text (assuming vectorizer and scaler are available globally)\n",
    "    try:\n",
    "        text_vector = vectorizer.transform([full_text])\n",
    "        numeric_scaled = scaler.transform(numeric_vector)\n",
    "        X = hstack([text_vector, csr_matrix(numeric_scaled)])\n",
    "    except NameError:\n",
    "        print(\"Warning: vectorizer or scaler not found. Using dummy features.\")\n",
    "        # Create dummy feature matrix for testing\n",
    "        X = np.zeros((1, 100))\n",
    "    \n",
    "    return X, feats, full_text\n",
    "\n",
    "def custom_email(raw_email):\n",
    "    \"\"\"Predict whether an email is phishing or benign\"\"\"\n",
    "    try:\n",
    "        # Extract features\n",
    "        X, feats, text = preprocess_raw_email(raw_email)\n",
    "        \n",
    "        # Predict (assuming model is available globally)\n",
    "        try:\n",
    "            prediction = model.predict(X)\n",
    "            probability = model.predict_proba(X)\n",
    "            \n",
    "            # Format results\n",
    "            label = \"Benign\" if prediction[0] == 0 else \"Phishing\"\n",
    "            prob_benign = probability[0][0]\n",
    "            prob_phishing = probability[0][1]\n",
    "            \n",
    "        except NameError:\n",
    "            print(\"Warning: model not found. Using dummy predictions.\")\n",
    "            # Dummy predictions for testing\n",
    "            label = \"Phishing\" if any(kw in text.lower() for kw in [\"urgent\", \"verify\", \"click here\"]) else \"Benign\"\n",
    "            prob_benign = 0.3 if label == \"Phishing\" else 0.8\n",
    "            prob_phishing = 0.7 if label == \"Phishing\" else 0.2\n",
    "        \n",
    "        return label, prob_benign, prob_phishing, feats\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in custom_email: {e}\")\n",
    "        return \"Error\", 0.5, 0.5, {}\n",
    "\n",
    "# ---------- Testing ----------\n",
    "emails_to_test = [\n",
    "    \"\"\"\n",
    "    Dear customer,\n",
    "    Your account has been locked due to suspicious activity.\n",
    "    Please verify your login at http://secure-banking-login.com immediately.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Hi Alice,\n",
    "    Just a reminder about our team meeting tomorrow at 10 AM in the office.\n",
    "    Cheers,\n",
    "    Bob\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Congratulations! You've won a $1000 gift card.\n",
    "    Click here to claim: http://free-reward-prize.info\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Dear student,\n",
    "    Here is the link to download the lecture notes for tomorrow's class:\n",
    "    https://university.edu/resources/notes/week3.pdf\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Subject: Urgent: Verify Your Account Now\n",
    "\n",
    "Dear Customer,\n",
    "\n",
    "We noticed unusual activity on your account. To avoid suspension please verify your credentials immediately:\n",
    "https://secure.example-phish.com/verify?user=123\n",
    "\n",
    "If you do not verify within 24 hours your account will be locked.\n",
    "\n",
    "Sincerely,\n",
    "Account Security Team\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Subject: Important: Confirm Recent Login\n",
    "\n",
    "Hi,\n",
    "\n",
    "We detected a login from a new device. If this wasn't you, please change your password now:\n",
    "http://update-login.example-phish.com/change-password\n",
    "\n",
    "Regards,\n",
    "Security Alerts\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Subject: Team Meeting Tomorrow — Agenda Attached\n",
    "\n",
    "Hi Team,\n",
    "\n",
    "Reminder: our weekly standup is tomorrow at 10:00 AM in Conference Room B.\n",
    "\n",
    "Agenda:\n",
    "- Project status\n",
    "- Blockers\n",
    "- Next steps\n",
    "\n",
    "Thanks,\n",
    "Priya\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Subject: Urgent: Verify Your Account Now\n",
    "\n",
    "Dear Customer,\n",
    "\n",
    "We detected unusual activity on your account. To avoid suspension, verify your identity immediately by visiting the following page: [REMOVE_LINK]. This verification must be completed within 24 hours.\n",
    "\n",
    "Regards,\n",
    "Account Security Team\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Pranav Rh, Urgent Requirement for the role of Intern\n",
    "Inbox\n",
    "\n",
    "Naukri Campus Jobs <recommendationnc@naukri.com>\n",
    "Fri 5 Sept, 13:30\n",
    "to me\n",
    "\n",
    "\n",
    "naukri.com\n",
    "Get App\n",
    "\t\n",
    "Your weekend opportunities are right here!\n",
    "\n",
    "Backend Developer Intern\n",
    "Quloi\t\t\t2.3\n",
    "\t\n",
    "6 months duration\n",
    "Internship\t\n",
    "\n",
    "Internship - Node.js\n",
    "Data Foundry\t\t\t3.4\n",
    "\t\n",
    "3 months duration\n",
    "Internship\t\n",
    "Jobs outside your preference\n",
    "\n",
    "Information Technology Intern\n",
    "Morgan Placement\t\n",
    "\t\n",
    "3 months duration\n",
    "Internship\t\n",
    "Are these jobs relevant?\t\t\n",
    "Yes\n",
    "No\n",
    "\n",
    "Web Developer Intern\n",
    "Ebws Llp\t\n",
    "\t\n",
    "3 months duration\n",
    "Internship\t\n",
    "View All Recommendations\n",
    "Applies are a click away on the Naukri app\n",
    "Available on\t\t\n",
    "Get App\n",
    "\n",
    "Scan to download\n",
    "Facebook\t\tTwitter\t\tInstagram\n",
    "Unsubscribe\t\t\t\tReport a Problem\n",
    "You have received this mail because your e-mail ID is registered with Naukri.com. This is a system-generated e-mail regarding your Naukri account preferences, please don't reply to this message. The jobs sent in this mail have been posted by the clients of Naukri.com. And we have enabled auto-login for your convenience, you are strongly advised not to forward this email to protect your account from unauthorized access. IEIL has taken all reasonable steps to ensure that the information in this mailer is authentic. Users are advised to research bonafides of advertisers independently. Please do not pay any money to anyone who promises to find you a job. IEIL shall not have any responsibility in this regard. We recommend that you visit our Terms & Conditions and the Security Advice for more comprehensive information.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Subject: URGENT: Verify Your Account Immediately!\n",
    "From: support@secure-example.com\n",
    "Return-Path: <mailer@phish-example.com>\n",
    "Reply-To: accounts@phish-example.com\n",
    "Authentication-Results: mx.example.com; spf=fail smtp.mailfrom=phish-example.com; dkim=pass header.d=secure-example.com\n",
    "Date: Tue, 1 Oct 2025 12:00:00 +0000\n",
    "Content-Type: multipart/mixed; boundary=\"ABC123\"\n",
    "\n",
    "--ABC123\n",
    "Content-Type: text/html; charset=\"utf-8\"\n",
    "\n",
    "<html>\n",
    "  <body>\n",
    "    <p>Dear Customer,</p>\n",
    "    <p>YOUR ACCOUNT WILL BE LOCKED if you do not VERIFY your credentials immediately!</p>\n",
    "    <p>Click the link below to confirm your login:</p>\n",
    "    <ul>\n",
    "      <li><a href=\"http://malicious.example-phish.com/login\">www.secure-example.com</a></li>\n",
    "      <li>Or use this shortcut link: http://bit.ly/fake-login</li>\n",
    "      <li>IP login portal: http://192.168.10.10/verify</li>\n",
    "    </ul>\n",
    "    <form action=\"http://forms.example-phish.com/submit\" method=\"post\">\n",
    "      <input type=\"text\" name=\"user\"/>\n",
    "      <input type=\"password\" name=\"pass\"/>\n",
    "      <button>Submit</button>\n",
    "    </form>\n",
    "    <script>\n",
    "      window.location = \"http://js-redirect.example-phish.com/steal\";\n",
    "    </script>\n",
    "    <p>Thank you,<br/>Secure Support Team</p>\n",
    "  </body>\n",
    "</html>\n",
    "\n",
    "--ABC123\n",
    "Content-Disposition: attachment; filename=\"Invoice_999.docm\"\n",
    "Content-Type: application/msword\n",
    "Content-Transfer-Encoding: base64\n",
    "\n",
    "[BASE64_CONTENT_TRUNCATED]\n",
    "\n",
    "--ABC123--\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Subject: Urgent – Verify Your Account\n",
    "\n",
    "Dear user,  \n",
    "Your account has been flagged. Please click the link below to verify:  \n",
    "http://bit.ly/fake-login  \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "print(\"Testing Email Classification:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, raw_email in enumerate(emails_to_test, 1):\n",
    "    print(f\"\\n--- Email {i} ---\")\n",
    "    label, prob_benign, prob_phishing, feats = custom_email(raw_email)\n",
    "    \n",
    "    print(\"Email snippet:\", raw_email.strip()[:80] + \"...\")\n",
    "    print(f\"Prediction: {label}\")\n",
    "    print(f\"Probability (Benign): {prob_benign:.3f}\")\n",
    "    print(f\"Probability (Phishing): {prob_phishing:.3f}\")\n",
    "    \n",
    "    # Show some key features\n",
    "    key_features = [\"num_urls\", \"kw_verify\", \"kw_urgent\", \"kw_click_here\", \"uppercase_ratio\"]\n",
    "    relevant_feats = {k: v for k, v in feats.items() if k in key_features and v > 0}\n",
    "    if relevant_feats:\n",
    "        print(\"Key features:\", relevant_feats)\n",
    "    \n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf6013d1-01e0-494c-9276-6fd04497740a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlxtend in d:\\phishguard-ai\\.venv\\lib\\site-packages (0.23.4)\n",
      "Requirement already satisfied: scipy>=1.2.1 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from mlxtend) (1.16.2)\n",
      "Requirement already satisfied: numpy>=1.16.2 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from mlxtend) (2.3.3)\n",
      "Requirement already satisfied: pandas>=0.24.2 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from mlxtend) (2.3.3)\n",
      "Requirement already satisfied: scikit-learn>=1.3.1 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from mlxtend) (1.7.2)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from mlxtend) (3.10.7)\n",
      "Requirement already satisfied: joblib>=0.13.2 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from mlxtend) (1.5.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2025.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from scikit-learn>=1.3.1->mlxtend) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\phishguard-ai\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46a6ad7c-6bed-4da5-87a7-8f9f46e7c578",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "StackingClassifier.__init__() got an unexpected keyword argument 'passthrough'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     12\u001b[39m meta_clf = LogisticRegression()\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Stacking Classifier\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# You can use 'predict' or 'predict_proba' for the meta-model input.\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# 'use_probas=True' often gives better results.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m sclf = \u001b[43mStackingClassifier\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclassifiers\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclf_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclf_rf\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmeta_classifier\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmeta_clf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_probas\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassthrough\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Pass the original features to the meta-model as well\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\n\u001b[32m     23\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m sclf.fit(X_train, y_train)\n\u001b[32m     26\u001b[39m stacked_preds = sclf.predict(X_test)\n",
      "\u001b[31mTypeError\u001b[39m: StackingClassifier.__init__() got an unexpected keyword argument 'passthrough'"
     ]
    }
   ],
   "source": [
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have X_train, y_train, X_test, y_test from your original code\n",
    "# Base models\n",
    "clf_lr = LogisticRegression(max_iter=2000, class_weight='balanced', solver='saga')\n",
    "clf_rf = RandomForestClassifier(n_estimators=200, class_weight='balanced', n_jobs=-1, random_state=42)\n",
    "\n",
    "# Meta-model (a simple Logistic Regression works well)\n",
    "meta_clf = LogisticRegression()\n",
    "\n",
    "# Stacking Classifier\n",
    "# You can use 'predict' or 'predict_proba' for the meta-model input.\n",
    "# 'use_probas=True' often gives better results.\n",
    "sclf = StackingClassifier(\n",
    "    classifiers=[clf_lr, clf_rf],\n",
    "    meta_classifier=meta_clf,\n",
    "    use_probas=True,\n",
    "    passthrough=True, # Pass the original features to the meta-model as well\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "sclf.fit(X_train, y_train)\n",
    "stacked_preds = sclf.predict(X_test)\n",
    "stacked_probs = sclf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Stacked Ensemble Report:\")\n",
    "print(classification_report(y_test, stacked_preds, digits=4))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, stacked_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af83cc16-1e67-4a4d-9795-ee85fdea38c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
